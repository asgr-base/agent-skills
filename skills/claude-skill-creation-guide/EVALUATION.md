# 評価と反復的開発

## まず評価を構築

**広範なドキュメントを書く前に評価を作成します。** これにより、想像上のものではなく実際の問題をスキルが解決することを保証します。

### 評価駆動開発

1. **ギャップを特定**: スキルなしで代表的なタスクでClaudeを実行。特定の失敗や欠けているコンテキストを文書化
2. **評価を作成**: これらのギャップをテストする3つのシナリオを構築
3. **ベースラインを確立**: スキルなしでClaudeのパフォーマンスを測定
4. **最小限の指示を書く**: ギャップに対処し、評価に合格するのに十分なコンテンツのみを作成
5. **反復**: 評価を実行し、ベースラインと比較し、改善

このアプローチにより、実際の問題を解決し、実現しない可能性のある要件を予測するのではなく、実際の問題を解決することが保証されます。

### 評価構造の例

```json
{
  "skills": ["pdf-processing"],
  "query": "Extract all text from this PDF file and save it to output.txt",
  "files": ["test-files/document.pdf"],
  "expected_behavior": [
    "Successfully reads the PDF file using an appropriate PDF processing library or command-line tool",
    "Extracts text content from all pages in the document without missing any pages",
    "Saves the extracted text to a file named output.txt in a clear, readable format"
  ]
}
```

**注**: これはシンプルなテストルーブリックを持つデータ駆動評価の例です。現在、これらの評価を実行するための組み込みの方法は提供されていません。ユーザーは独自の評価システムを作成できます。評価はスキルの有効性を測定するための信頼できる情報源です。

## Claudeと反復的にスキルを開発

最も効果的なスキル開発プロセスは、Claude自体を含みます。1つのClaudeインスタンス（「Claude A」）と協力してスキルを作成し、他のインスタンス（「Claude B」）が実際のタスクで使用します。

Claude Aは効果的なエージェント指示の書き方とエージェントが必要とする情報の両方を理解しているため、これが機能します。

### 新しいスキルの作成

1. **スキルなしでタスクを完了**: 通常のプロンプトでClaude Aと問題を解決。作業中、コンテキスト、好み、手順知識を自然に提供。繰り返し提供する情報に注目

2. **再利用可能なパターンを特定**: タスク完了後、将来の類似タスクに役立つコンテキストを特定

   **例**: BigQuery分析を行った場合、テーブル名、フィールド定義、フィルタリングルール（「常にテストアカウントを除外」など）、一般的なクエリパターンを提供したかもしれません

3. **Claude Aにスキルを作成するよう依頼**:

   "Create a Skill that captures this BigQuery analysis pattern we just used. Include the table schemas, naming conventions, and the rule about filtering test accounts."

   **Tip**: Claudeモデルはスキル形式と構造をネイティブに理解しています。Claudeにスキルを作成するように依頼するだけで、適切なフロントマターと本文コンテンツを持つ適切に構造化されたSKILL.mdコンテンツを生成します。

4. **簡潔さをレビュー**: Claude Aが不要な説明を追加していないか確認。例：「勝率が何を意味するかについての説明を削除してください - Claudeは既にそれを知っています。」

5. **情報アーキテクチャを改善**: Claude Aにコンテンツをより効果的に整理するよう依頼。例：「テーブルスキーマが別の参照ファイルにあるように整理してください。後でテーブルを追加する可能性があります。」

6. **類似タスクでテスト**: Claude B（スキルが読み込まれた新しいインスタンス）と関連ユースケースでスキルを使用。Claude Bが正しい情報を見つけ、ルールを正しく適用し、タスクを成功裏に処理するかどうかを観察

7. **観察に基づいて反復**: Claude Bが苦労したり何かを見逃したりした場合、具体的にClaude Aに戻る：「Claudeがこのスキルを使用したとき、Q4の日付でフィルタリングするのを忘れました。日付フィルタリングパターンについてのセクションを追加すべきですか？」

### 既存スキルの反復

同じ階層パターンが続きます。以下の間を交互に行います：

- **Claude Aと作業**（スキルを改良する専門家）
- **Claude Bでテスト**（スキルを使用して実際の作業を行うエージェント）
- **Claude Bの動作を観察**し、洞察をClaude Aに持ち帰る

1. **実際のワークフローでスキルを使用**: Claude B（スキルが読み込まれた）に実際のタスクを与える（テストシナリオではなく）

2. **Claude Bの動作を観察**: 苦労する場所、成功する場所、または予期しない選択をする場所に注目

   **観察例**: 「Claude Bに地域別売上レポートを依頼したとき、クエリを書きましたが、スキルがこのルールに言及しているにもかかわらず、テストアカウントをフィルタリングするのを忘れました。」

3. **改善のためClaude Aに戻る**: 現在のSKILL.mdと観察したことを共有。「Claude Bが地域レポートを依頼したときにテストアカウントをフィルタリングするのを忘れたことに気づきました。スキルはフィルタリングに言及していますが、十分に目立たないのかもしれません？」

4. **Claude Aの提案をレビュー**: Claude Aは以下を提案するかもしれません：
   - ルールをより目立つように再編成
   - 「always filter」の代わりに「MUST filter」のようなより強い言語を使用
   - ワークフローセクションの再構築

5. **変更を適用してテスト**: Claude Aの改良でスキルを更新し、類似のリクエストでClaude Bと再度テスト

6. **使用に基づいて繰り返し**: 新しいシナリオに遭遇するたびに継続。各反復は、仮定ではなく実際のエージェント動作に基づいてスキルを改善

### チームフィードバックの収集

1. チームメイトとスキルを共有し、使用を観察
2. 質問：スキルは期待通りに起動しますか？指示は明確ですか？何が欠けていますか？
3. フィードバックを組み込んで、自分の使用パターンの盲点に対処

### このアプローチが機能する理由

- Claude Aはエージェントのニーズを理解
- あなたはドメインの専門知識を提供
- Claude Bは実際の使用を通じてギャップを明らかにする
- 反復的な改良は、仮定ではなく観察された動作に基づいてスキルを改善

## 複数モデルでのテスト

スキルはモデルへの追加として機能するため、効果は基礎となるモデルに依存します。使用予定のすべてのモデルでスキルをテストしてください。

### モデル別のテスト考慮事項

- **Claude Haiku**（高速、経済的）: スキルは十分なガイダンスを提供していますか？
- **Claude Sonnet**（バランス）: スキルは明確で効率的ですか？
- **Claude Opus**（強力な推論）: スキルは過度に説明していませんか？

Opusで完璧に機能するものは、Haikuにはより詳細が必要かもしれません。複数のモデルでスキルを使用する予定がある場合は、すべてでうまく機能する指示を目指してください。

---

**参照元**: [SKILL.md](SKILL.md)
